1. 为什么需要 FP8 GEMM：减少存储，提高计算速度
2. FP8 数据结构（符号位+指数位+尾数位）：
    2.1 E4M3：4位指数 + 3位尾数。精度相对较高，但范围窄（最大值 448）。常用于前向传播（权重和激活值）。
    2.2 E5M2：5位指数 + 2位尾数。精度较低，但范围广（类似缩减版 BF16）。常用于反向传播（梯度）。
3. 量化的本质是通过一个 Scale（缩放因子） 将高动态范围的数据映射到 FP8 的有效表示区间内。公式见：
    /home/zfx/AI_infra/trition/fp8_gemm/公式.png
4. FP8 GEMM 并不是先将 FP8 转回 FP32 再算，而是在 FP8 下计算，在 FP32 下累加：
    4.1 Load (FP8)：从显存加载 FP8 格式的矩阵块到寄存器。
    4.2 Multiply (FP8)：Tensor Core 内部的乘法器以 FP8格式计算。但是计算的结果是FP32格式（保存到寄存器），这是防止求和过程中的溢出（Overflow）和精度掉落（由于尾数太短导致的数值“吃掉”现象）。
    4.4 在最后阶段，将 FP32 累加器的结果乘以 Scale。根据需要写回显存为 FP32、FP16 或 BF16。
5. FP8*FP8的结果以FP32的格式保存在寄存器中